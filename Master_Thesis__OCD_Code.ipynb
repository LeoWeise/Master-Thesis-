{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cb304b-dcd0-44e0-a90c-416bb09aed83",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Step 1: Data Scraping \n",
    "## Using the PRAW API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808242c1-8cf8-4814-a8c5-b20c6008a637",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up Reddit instance\n",
    "client_id = 'xxxxxxxxxxxxxxxxxxx'\n",
    "user_agent = 'xxxxxxxxxxxxx'\n",
    "client_secret = 'xxxxxxxxxxxxxxxx'\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "# Save data\n",
    "def save_data(posts, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(posts, file, indent=4)\n",
    "\n",
    "# List to store post data\n",
    "post_data = []\n",
    "\n",
    "# Generate file name with timestamp\n",
    "filename = f\"reddit_data_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "\n",
    "# Get 1000 hot posts\n",
    "hot_posts = reddit.subreddit('OCD').hot(limit=1000)\n",
    "\n",
    "# Define missing posts\n",
    "missing_posts = 0\n",
    "\n",
    "# Count posts to know when 100 are reached \n",
    "for index, post in enumerate(hot_posts):\n",
    "    try:\n",
    "        print('title:', post.title)\n",
    "        print('text:', post.selftext)\n",
    "        print('number of comments:', post.num_comments)\n",
    "\n",
    "        # define deleted posts\n",
    "        is_deleted = post.selftext == \"[deleted]\"\n",
    "\n",
    "        # get all comments for the posts (not printed, just saved)\n",
    "        post.comments.replace_more(limit=None)\n",
    "        comments = [comment.body for comment in post.comments.list()]\n",
    "\n",
    "        # Structure data for each post -> store it in a file\n",
    "        post_info = {\n",
    "            'title': post.title,\n",
    "            'selftext': post.selftext,\n",
    "            'comments': comments,\n",
    "            'num_comments': post.num_comments,\n",
    "            'stickied': post.stickied,\n",
    "            'deleted': is_deleted,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'score': post.score,\n",
    "            'created_utc': post.created_utc,\n",
    "            'removed_by_category': post.__dict__.get(\"removed_by_category\", None),\n",
    "        }\n",
    "        # Add to the list of post data\n",
    "        post_data.append(post_info)\n",
    "\n",
    "        # Save progress every 100 posts for case of error \n",
    "        if (index + 1) % 100 == 0:\n",
    "            save_data(post_data, filename)\n",
    "            print(f\"âœ… Progress saved after {index + 1} posts\")\n",
    "\n",
    "        # Add a 4-second-delay between requests (to avoid hitting the rate limit)\n",
    "        time.sleep(4)\n",
    "\n",
    "    # in case of error\n",
    "    except Exception as e: \n",
    "        print(f\"Error encountered: {e}. Saving progress...\") # (optional)\n",
    "        save_data(post_data, filename)\n",
    "        break  # Stop scraping to prevent further issues\n",
    "\n",
    "# Save the data to a JSON file\n",
    "save_data(post_data, filename)\n",
    "\n",
    "# Report the number of posts scraped (optional)\n",
    "print(f\"Total posts scraped: {len(post_data)}\")\n",
    "print(f\"Total missing (deleted/removed) posts: {missing_posts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6cae37-b8d0-4aa4-88e0-bf7ebb4ac3dd",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Combine all scraped data into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b0755-afad-474c-a6f6-9f4beebe5076",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "folder_path = \"/Users/leonaweise/PycharmProjects/Thesis/LeonaThesisProject\"\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith(\".json\")]\n",
    "\n",
    "combined_posts = {}\n",
    "skipped_files = []\n",
    "\n",
    "for filename in files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            for post in data:\n",
    "                title = post.get(\"title\", \"\").strip()\n",
    "                body = post.get(\"selftext\", \"\").strip()\n",
    "                comments = post.get(\"comments\", [])\n",
    "                key = title + body\n",
    "\n",
    "                if key in combined_posts:\n",
    "                    # If new version has more comments, replace it\n",
    "                    if len(comments) > len(combined_posts[key][\"comments\"]):\n",
    "                        combined_posts[key] = post\n",
    "                else:\n",
    "                    combined_posts[key] = post\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped_files.append((filename, str(e)))\n",
    "        \n",
    "# Prepare output file\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_file = f\"combined_reddit_data_{timestamp}.json\"\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(folder_path, output_file), \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(list(combined_posts.values()), file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Combined and saved {len(combined_posts)} unique posts to: {output_file}\")\n",
    "if skipped_files:\n",
    "    print(\"Skipped files due to errors:\")\n",
    "    for name, reason in skipped_files:\n",
    "        print(f\" - {name}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad356b-8e0e-41af-bf89-69eb9b0d7c7e",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Step 2: Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56996-5206-4c3b-88d8-2bb16b498c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "What was removed?\n",
    "1. 'stickied' posts\n",
    "2. 2 types of bot-generated comments\n",
    "    \"If you or someone you know is contemplating suicide, please do not hesitate to talk to someone. The wonderful u/froidinslip has \\nwritten an invaluable post to help you navigate this time: \\nhttps://www.reddit.com/r/OCD/comments/q4zeo1/please_read_this_before_posting_about_feeling/ \\nYou are not alone, and you have options. However, we are not able to help with suicide on an internet forum. \\nPLEASE USE THE RESOURCES. You matter and deserve help.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/OCD) if you have any questions or concerns.*\"\n",
    "    \"This post has been automatically marked as \\\"spoiler\\\" and \\\"NSFW\\\", due to the nature of the content (and in accordance with subreddit rule number 4 if this post has been flaired as \\\"Crisis\\\").\\n\\n(This subreddit uses the \\\"spoiler\\\" and \\\"NSFW\\\" markers to hide a post's content behind an expandable/collapsible wall. It does not imply that the content contains actual spoiler or NSFW content, and the post will remain publicly-visible.)\\n\\n**Do not remove the \\\"spoiler\\\" and \\\"NSFW\\\" markers without permission from the moderators.** Failure to comply can and will result in this post being removed.\\n\\nThe cooperation in making this subreddit an accessible community for all will be appreciated.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/OCD) if you have any questions or concerns.*\"\n",
    "3. [removed] comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5c94b-31ff-480b-85b9-1f1c452a64bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "with open(\n",
    "        \"/Users/leonaweise/PycharmProjects/Thesis/LeonaThesisProject/combined_reddit_data_2025-04-13.json\",\n",
    "        encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Remove stickied posts\n",
    "non_stickied_data = [post for post in data if not post.get('stickied', False)]\n",
    "stickied_removed = len(data) - len(non_stickied_data)\n",
    "\n",
    "# Counter for each type of comment removed\n",
    "spoiler_bot_comments = 0\n",
    "suicide_bot_comments = 0\n",
    "removed_comments = 0\n",
    "\n",
    "# Clean comments \n",
    "cleaned_data = []\n",
    "\n",
    "for post in non_stickied_data:\n",
    "    cleaned_post = post.copy()\n",
    "\n",
    "    if 'comments' in cleaned_post and isinstance(cleaned_post['comments'], list):\n",
    "        cleaned_comments = []\n",
    "\n",
    "        for comment in cleaned_post['comments']:\n",
    "            if not isinstance(comment, str):\n",
    "                cleaned_comments.append(comment)\n",
    "                continue\n",
    "\n",
    "            if comment.startswith(\"This post has been automatically marked as \\\"spoiler\\\" and \\\"NSFW\\\"\"):\n",
    "                spoiler_bot_comments += 1\n",
    "            elif comment.startswith(\"If you or someone you know is contemplating suicide\"):\n",
    "                suicide_bot_comments += 1\n",
    "            elif comment == \"[removed]\":\n",
    "                removed_comments += 1\n",
    "            else:\n",
    "                cleaned_comments.append(comment)\n",
    "\n",
    "        cleaned_post['comments'] = cleaned_comments\n",
    "\n",
    "    cleaned_data.append(cleaned_post)\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"final_cleaned_reddit_data_2025_04_13.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(cleaned_data, file)\n",
    "    \n",
    "# Define removed comments\n",
    "total_comments_removed = spoiler_bot_comments + suicide_bot_comments + removed_comments\n",
    "\n",
    "# (optional) \n",
    "print(f\"Done. Processing results:\")\n",
    "print(f\"- Removed {stickied_removed} stickied posts\")\n",
    "print(f\"- Removed {spoiler_bot_comments} spoiler/NSFW bot comments\")\n",
    "print(f\"- Removed {suicide_bot_comments} suicide prevention bot comments\")\n",
    "print(f\"- Removed {removed_comments} [removed] comments\")\n",
    "print(f\"- Total comments removed: {total_comments_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab5b2-65d1-481d-9702-50a649999733",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Step 3: Topic Modelling\n",
    "## Using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62dfc4-b015-4e92-951b-d88dd387d397",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# 1. Load data as Dataframe\n",
    "with open(\"//Users/leonaweise/PycharmProjects/Thesis/LeonaThesisProject/final_cleaned_reddit_data_2025_04_13.json\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Prepare documents: Combining title, body and comments\n",
    "df[\"whole_post\"] = df['title'].fillna(\"\") + \" \" + df[\"selftext\"].fillna(\"\") + \" \" + df['comments'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "docs = df[\"whole_post\"].tolist()\n",
    "\n",
    "# 3. BERTopic Pipeline\n",
    "# Embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# Dimensionality reduction\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=3,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Clustering model\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True,\n",
    "    min_samples=10\n",
    ")\n",
    "\n",
    "# Vectorizer model\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "# c-TF-IDF model\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "\n",
    "# Topic representation model (optional, Grootendorst sometimes uses this)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# 4. Initialize BERTopic with components\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=10,  # Show more words per topic\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. Fit model\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Save raw topics and probabilities for reuse (before reduction)\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "with open(f\"raw_topics_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(topics.tolist() if isinstance(topics, np.ndarray) else topics, f)\n",
    "np.save(f\"raw_probs_{timestamp}.npy\", probs)\n",
    "\n",
    "# 6. Reduce final topics\n",
    "topic_model.reduce_topics(docs, nr_topics=35)\n",
    "reduced_topics, reduced_probs = topic_model.transform(docs)\n",
    "\n",
    "# Create topic-term matrix\n",
    "topic_term_matrix = topic_model.get_topics()\n",
    "with open(f\"topic_term_matrix_{timestamp}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topic_term_matrix, f)\n",
    "\n",
    "# Save term ranks\n",
    "term_ranks = {}\n",
    "for topic_id in set(reduced_topics):\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        # Get topic terms with weights\n",
    "        terms_with_weights = topic_model.get_topic(topic_id)\n",
    "        # Convert to dictionary format for easier analysis\n",
    "        term_ranks[int(topic_id)] = {word: float(weight) for word, weight in terms_with_weights}\n",
    "\n",
    "# Save to JSON for easy access\n",
    "with open(f\"topic_term_ranks_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(term_ranks, f)\n",
    "\n",
    "# 6.1 Calculate topic coherence for reduced topics\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Prepare text data for coherence calculation\n",
    "print(\"Preparing data for coherence calculation...\")\n",
    "# Properly tokenize documents (simple whitespace split may not be enough)\n",
    "# First, make sure all docs are strings and lowercase for consistency\n",
    "string_docs = [str(doc).lower() for doc in docs]\n",
    "# Then tokenize - this is a simple approach, you might want to use nltk or spacy for better tokenization\n",
    "processed_docs = [doc.split() for doc in string_docs]\n",
    "# Create a Dictionary from processed docs\n",
    "id2word = corpora.Dictionary(processed_docs)\n",
    "# Filter extremely rare words (optional but recommended)\n",
    "id2word.filter_extremes(no_below=3, no_above=0.5)\n",
    "# Create corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Get topic words for reduced topics\n",
    "print(\"Extracting topic words...\")\n",
    "n_words = 10\n",
    "topics_dict = topic_model.get_topics()\n",
    "topic_words = []\n",
    "skipped_topics = []\n",
    "\n",
    "# Make sure to properly prepare topic words for coherence calculation\n",
    "for topic_id in sorted(topics_dict.keys()):\n",
    "    if topic_id == -1:  # Skip outlier topic\n",
    "        continue\n",
    "\n",
    "    topic = topic_model.get_topic(topic_id)\n",
    "    if topic:\n",
    "        # Extract just the words (without weights)\n",
    "        words = [word for word, _ in topic[:n_words]]\n",
    "        # For each topic term, split it if it contains a space\n",
    "        words_expanded = []\n",
    "        for word in words:\n",
    "            if \" \" in word:\n",
    "                words_expanded.extend(word.split())\n",
    "            else:\n",
    "                words_expanded.append(word)\n",
    "        words = words_expanded\n",
    "\n",
    "        # Important: Verify that all words are in the dictionary\n",
    "        # Debug which words are missing from dictionary\n",
    "        for word in words:\n",
    "            if word not in id2word.token2id:\n",
    "                print(f\"Topic {topic_id}: Term '{word}' not in dictionary\")\n",
    "        # Filter out words that aren't in the id2word dictionary\n",
    "        words_in_vocab = [word for word in words if word in id2word.token2id]\n",
    "\n",
    "        if len(words_in_vocab) >= 3:  # Need at least a few words for coherence\n",
    "            topic_words.append(words_in_vocab)\n",
    "        else:\n",
    "            skipped_topics.append(topic_id)\n",
    "            print(f\"Warning: Topic {topic_id} has too few words in vocabulary: {words_in_vocab}\")\n",
    "    else:\n",
    "        skipped_topics.append(topic_id)\n",
    "        print(f\"Warning: Topic {topic_id} is empty\")\n",
    "\n",
    "# Verify we have topics to calculate coherence on\n",
    "if not topic_words:\n",
    "    print(\"ERROR: No valid topics found for coherence calculation!\")\n",
    "    coherence_results = {\n",
    "        'cv_coherence': None,\n",
    "        'npmi_coherence': None,\n",
    "        'num_topics': 0,\n",
    "        'error': 'No valid topics for coherence calculation'\n",
    "    }\n",
    "else:\n",
    "    # Debug print to help diagnose issues\n",
    "    print(f\"First topic words sample: {topic_words[0]}\")\n",
    "    print(f\"Number of topics for coherence: {len(topic_words)}\")\n",
    "\n",
    "    # Calculate coherence metrics - make sure topics and texts match format\n",
    "    print(\"Calculating coherence scores...\")\n",
    "    try:\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=processed_docs,\n",
    "            dictionary=id2word,\n",
    "            coherence='c_v',\n",
    "            processes=1  # Force single process mode\n",
    "        ).get_coherence()\n",
    "\n",
    "        coherence_npmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=processed_docs,\n",
    "            dictionary=id2word,\n",
    "            coherence='c_npmi',\n",
    "            processes=1  # Force single process mode\n",
    "        ).get_coherence()\n",
    "\n",
    "        # Save coherence results\n",
    "        coherence_results = {\n",
    "            'cv_coherence': float(coherence_cv),\n",
    "            'npmi_coherence': float(coherence_npmi),\n",
    "            'num_topics': len(topic_words)\n",
    "        }\n",
    "\n",
    "        print(\"Coherence scores:\")\n",
    "        for metric, value in coherence_results.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in coherence calculation: {e}\")\n",
    "        # Provide a fallback\n",
    "        coherence_results = {\n",
    "            'error': str(e),\n",
    "            'num_topics': len(topic_words)\n",
    "        }\n",
    "\n",
    "if skipped_topics:\n",
    "    print(\"Skipped empty or invalid topics:\", skipped_topics)\n",
    "else:\n",
    "    print(\"All topics were valid for coherence calculation.\")\n",
    "\n",
    "with open(f\"coherence_metrics_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(coherence_results, f)\n",
    "\n",
    "# Quick topic size histogram\n",
    "topic_counts = pd.Series(reduced_topics).value_counts()\n",
    "topic_counts = topic_counts[topic_counts.index != -1]  # Remove outlier topic\n",
    "topic_counts.sort_index().plot(kind='bar', figsize=(15, 5))\n",
    "plt.title('Number of Documents per Topic')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f\"topic_size_histogram_{timestamp}.png\", dpi=300)\n",
    "\n",
    "# 7. Create DataFrame with results\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(f\"reduced_topic_info_{timestamp}.csv\", index=False)\n",
    "probabilities = probs.max(axis=1) if len(probs.shape) > 1 else np.ones(len(topics))\n",
    "\n",
    "# After getting reduced topics, create a topic-to-documents mapping\n",
    "# Create a mapping of topics to documents with counts and probability stats\n",
    "# Create a mapping of topics to documents with counts and probability stats\n",
    "topic_to_docs = {}\n",
    "\n",
    "for topic_id in set(reduced_topics):\n",
    "    # Skip outlier topic if desired\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "\n",
    "    # Get all documents for this topic\n",
    "    topic_docs_indices = [i for i, t in enumerate(reduced_topics) if t == topic_id]\n",
    "\n",
    "    # Document count for this topic\n",
    "    doc_count = len(topic_docs_indices)\n",
    "\n",
    "    # Collect all probabilities for this topic\n",
    "    topic_probabilities = []\n",
    "\n",
    "    # Create a list of documents with their full content\n",
    "    topic_documents = []\n",
    "    for idx in topic_docs_indices:\n",
    "        # Get probability for this document-topic pair\n",
    "        if isinstance(reduced_probs[idx], np.ndarray):\n",
    "            doc_prob = float(reduced_probs[idx][topic_id])\n",
    "        else:\n",
    "            doc_prob = float(reduced_probs[idx])\n",
    "\n",
    "        topic_probabilities.append(doc_prob)\n",
    "\n",
    "        # Get number of comments (either from field or by counting)\n",
    "        num_comments = data[idx].get(\"num_comments\")\n",
    "        if num_comments is None:\n",
    "            num_comments = len(data[idx].get(\"comments\", []))\n",
    "\n",
    "        # Include the full document content with topic probability\n",
    "        doc_info = {\n",
    "            'title': data[idx].get('title', ''),\n",
    "            'selftext': data[idx].get('selftext', ''),\n",
    "            'comments': data[idx].get('comments', []),\n",
    "            'num_comments': num_comments,\n",
    "            'topic_probability': doc_prob  # Document's probability for this topic\n",
    "        }\n",
    "        topic_documents.append(doc_info)\n",
    "\n",
    "    # Sort by probability (most representative first)\n",
    "    topic_documents.sort(key=lambda x: x['topic_probability'], reverse=True)\n",
    "\n",
    "    # Calculate probability statistics\n",
    "    avg_probability = sum(topic_probabilities) / max(1, len(topic_probabilities))\n",
    "\n",
    "    # Store everything for this topic\n",
    "    topic_to_docs[int(topic_id)] = {\n",
    "        'documents': topic_documents,\n",
    "        'document_count': doc_count,\n",
    "        'avg_topic_probability': avg_probability,\n",
    "        'keywords': [word for word, _ in topic_model.get_topic(topic_id)][:10]  # Top 10 keywords\n",
    "    }\n",
    "\n",
    "# Save all documents by topic\n",
    "with open(f\"documents_by_topic_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(topic_to_docs, f)\n",
    "\n",
    "# 9. Visualizations\n",
    "fig_heatmap = topic_model.visualize_heatmap()\n",
    "fig_heatmap.write_html(f\"topic_document_heatmap_{timestamp}.html\")\n",
    "\n",
    "fig_topics = topic_model.visualize_topics()\n",
    "fig_topics.write_html(f\"intertopic_distance_map_{timestamp}.html\")\n",
    "\n",
    "fig_barchart = topic_model.visualize_barchart(top_n_topics=50)\n",
    "fig_barchart.write_html(f\"topic_barchart_{timestamp}.html\")\n",
    "\n",
    "fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "fig_hierarchy.write_html(f\"topic_hierarchy_{timestamp}.html\")\n",
    "\n",
    "# 10. Save model\n",
    "topic_model.save(f\"bertopic_model_{timestamp}\")\n",
    "\n",
    "print(f\"Analysis complete. Found {len(topic_info) - 1} topics.\")\n",
    "print(f\"Topic information:\")\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bea23-de32-432a-a9cd-72053a995412",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create labels Using GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d729a9d-4aca-4e9d-aaff-b08c6a202632",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "import openai\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Set the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# Load topic keywords from CSV\n",
    "df_keywords = pd.read_csv(\"reduced_topic_info_2025-04-22_22-52-49.csv\")\n",
    "df_keywords = df_keywords[df_keywords[\"Topic\"] >= 0].copy()\n",
    "df_keywords[\"Parsed_Keywords\"] = df_keywords[\"Representation\"].apply(ast.literal_eval)\n",
    "topic_to_keywords = dict(zip(df_keywords[\"Topic\"], df_keywords[\"Parsed_Keywords\"]))\n",
    "\n",
    "# Load the full thesis document summary\n",
    "df_full_summary = pd.read_csv(\"Thesis_Topic_Document_Summary_FULL.csv\")\n",
    "\n",
    "# Create OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Detailed prompt for OCD topic labeling\n",
    "prompt_template = \"\"\"You will receive topic keywords and full Reddit posts (including titles, selftexts, and comments).\n",
    "Your task is to generate a short, accurate, and uniquely descriptive topic label for each topic, \n",
    "that captures the specific OCD theme, concern, or experience in this topic.\n",
    "\n",
    "You are analyzing a topic from an OCD subreddit. The topic contains posts with these keywords:\n",
    "{keywords}\n",
    "\n",
    "Here are sample posts from this topic:\n",
    "{sample_posts}\n",
    "\n",
    "Follow these strict rules:\n",
    "- Format: topic: <label>\n",
    "- The label must be no more than 4 words\n",
    "- Do not repeat labels across topics â€” each label must be semantically distinct\n",
    "- Avoid vague or generic labels like \"topic 2\", or \"miscellaneous\"\n",
    "- Base the label primarily on the post content, not just the keywords\n",
    "- Consider the types of distress, compulsions, fears, or themes being expressed\n",
    "- Use natural language, not diagnostic or clinical jargon unless reflected in the posts\n",
    "- Focus on the specific fears, compulsions, or themes expressed\n",
    "\n",
    "Examples of good labels: 'Religious and Moral Scrupulosity', 'Memory issues', 'OCD Treatment', 'Pregnancy-related OCD',\n",
    " 'Numerical OCD', 'Pregnancy-Related Intrusive Thoughts'\n",
    "\n",
    "Respond only with the label, without quotation marks or explanation.\"\"\"\n",
    "\n",
    "# Process each topic individually\n",
    "topic_labels = {}\n",
    "valid_topics = sorted(topic_to_keywords.keys())\n",
    "total_topics = len(valid_topics)\n",
    "\n",
    "for index, topic_id in enumerate(valid_topics, start=1):\n",
    "    print(f\"\\nProcessing Topic {topic_id} ({index}/{total_topics})...\")\n",
    "\n",
    "    # Get keywords for this topic\n",
    "    keywords = topic_to_keywords[topic_id]\n",
    "    keywords_str = \", \".join(keywords[:15])  \n",
    "\n",
    "    # Get all posts for this topic\n",
    "    topic_docs = df_full_summary[(df_full_summary[\"topic_number\"] == topic_id) & df_full_summary[\"post_title\"].notna()]\n",
    "    sample_posts = []\n",
    "\n",
    "    # Use all available posts for each topic (no limit)\n",
    "    for _, doc in topic_docs.iterrows():\n",
    "        title = doc[\"post_title\"]\n",
    "        selftext = doc[\"post_body\"]\n",
    "        comments = doc[\"comments\"]\n",
    "\n",
    "        # Create a combined sample with title, body, and comments\n",
    "        sample = f\"POST TITLE: {title}\\nPOST CONTENT: {selftext}\\nCOMMENTS:\\n{comments}\"\n",
    "        sample_posts.append(sample)\n",
    "\n",
    "    # Combine samples for the final prompt\n",
    "    sample_posts_str = \"\\n\".join(sample_posts)\n",
    "\n",
    "    # Create the complete prompt\n",
    "    prompt = prompt_template.format(\n",
    "        keywords=keywords_str,\n",
    "        sample_posts=sample_posts_str\n",
    "    )\n",
    "\n",
    "    # Call OpenAI API directly\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"You are a helpful assistant specializing in psychology and mental health.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=10  # Keep responses short\n",
    "        )\n",
    "\n",
    "        # Extract the label\n",
    "        label = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Store the label\n",
    "        topic_labels[topic_id] = label\n",
    "        print(f\"  Topic {topic_id} label: {label}\")\n",
    "\n",
    "        # Wait between API calls to avoid rate limits\n",
    "        time.sleep(15)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing Topic {topic_id}: {str(e)}\")\n",
    "\n",
    "        # Wait longer if rate limited\n",
    "        if \"rate_limit\" in str(e).lower():\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            time.sleep(15)\n",
    "\n",
    "# Update model with new labels\n",
    "if topic_labels:\n",
    "    print(f\"\\nUpdating model with {len(topic_labels)} new topic labels...\")\n",
    "\n",
    "    # Save just the topic labels to a CSV file\n",
    "    topic_labels_df = pd.DataFrame({\n",
    "        'Topic': list(topic_labels.keys()),\n",
    "        'Label': list(topic_labels.values())\n",
    "    })\n",
    "    topic_labels_df.to_csv(\"topic_labels_only.csv\", index=False)\n",
    "    print(\"Saved raw topic labels to topic_labels_only.csv\")\n",
    "\n",
    "    topic_model = BERTopic.load(\"bertopic_model_2025-04-22_22-52-49\")\n",
    "    topic_model.set_topic_labels(topic_labels)\n",
    "    print(\"Labels applied successfully\")\n",
    "\n",
    "    # Save the updated model\n",
    "    topic_model.save(\"TOPICS_updated_with_gpt4o_labels3\")\n",
    "    print(\"Updated model saved as 'TOPICS_updated_with_gpt4o_labels2'\")\n",
    "\n",
    "    # Export topic info to CSV\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_info.to_csv(\"GPT4o_Topic_Labels2.csv\", index=False)\n",
    "    print(\"Saved topic labels to GPT4o_Topic_Labels2.csv\")\n",
    "else:\n",
    "    print(\"No labels were generated, model not updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160cefe-64eb-40df-8005-e2d333d505c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create an overview of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51177ea4-08de-46ea-a4c5-41a3146ba28b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load topic-to-documents JSON\n",
    "with open(\"documents_by_topic_2025-04-22_22-52-49.json\", \"r\") as f:\n",
    "    documents_by_topic = json.load(f)\n",
    "\n",
    "# Load topic information\n",
    "topic_model = BERTopic.load(\"bertopic_model_2025-04-22_22-52-49\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "label_map = dict(zip(topic_info.Topic, topic_info.Name))\n",
    "\n",
    "# Load keywords\n",
    "df_keywords = pd.read_csv(\"reduced_topic_info_2025-04-22_22-52-49.csv\")\n",
    "df_keywords[\"Parsed_Keywords\"] = df_keywords[\"Representation\"].apply(ast.literal_eval)\n",
    "\n",
    "# Build grouped CSV structure\n",
    "rows = []\n",
    "for _, row in df_keywords.iterrows():\n",
    "    topic_id = row[\"Topic\"]\n",
    "\n",
    "    # Skip the -1 topic (outlier group)\n",
    "    if topic_id == -1:\n",
    "        print(f\"Skipping outlier topic -1\")\n",
    "        continue\n",
    "\n",
    "    topic_label = label_map.get(topic_id, \"UNLABELED\")\n",
    "    topic_keywords = \", \".join(row[\"Parsed_Keywords\"])\n",
    "    documents = documents_by_topic.get(str(topic_id), {}).get(\"documents\", [])\n",
    "\n",
    "    # Calculate average topic probability\n",
    "    if documents:\n",
    "        avg_prob = round(sum(doc.get(\"topic_probability\", 0) for doc in documents) / len(documents), 4)\n",
    "    else:\n",
    "        avg_prob = 0.0\n",
    "\n",
    "    # Sort documents by topic probability (highest to lowest)\n",
    "    sorted_docs = sorted(documents, key=lambda doc: doc.get(\"topic_probability\", 0), reverse=True)\n",
    "\n",
    "    # Add topic header row\n",
    "    rows.append({\n",
    "        \"topic_number\": topic_id,\n",
    "        \"topic_label\": topic_label,\n",
    "        \"topic_keywords\": topic_keywords,\n",
    "        \"average_topic_probability\": avg_prob,\n",
    "        \"document_count\": len(documents),\n",
    "        \"post_title\": \"\",\n",
    "        \"post_body\": \"\",\n",
    "        \"comments\": \"\",\n",
    "        \"number_of_comments\": \"\",\n",
    "        \"topic_probability\": \"\"\n",
    "    })\n",
    "\n",
    "    # Add sorted document rows\n",
    "    for doc in sorted_docs:\n",
    "        title = doc.get(\"title\", \"\")\n",
    "        selftext = doc.get(\"selftext\", \"\")\n",
    "\n",
    "        # Add numbered comments with line breaks\n",
    "        comments_list = doc.get(\"comments\", [])\n",
    "        comments = \"\\n\".join([f\"{i+1}. {comment.strip()}\" for i, comment in enumerate(comments_list)])\n",
    "\n",
    "        comment_count = doc.get(\"num_comments\", len(comments_list))\n",
    "        topic_prob = round(doc.get(\"topic_probability\", 0), 6)  # Use 6 decimals for consistency\n",
    "\n",
    "        rows.append({\n",
    "            \"topic_number\": \"\",\n",
    "            \"topic_label\": \"\",\n",
    "            \"topic_keywords\": \"\",\n",
    "            \"average_topic_probability\": \"\",\n",
    "            \"document_count\": \"\",\n",
    "            \"post_title\": title,\n",
    "            \"post_body\": selftext,\n",
    "            \"comments\": comments,\n",
    "            \"number_of_comments\": comment_count,\n",
    "            \"topic_probability\": topic_prob\n",
    "        })\n",
    "\n",
    "# Export to CSV\n",
    "final_df = pd.DataFrame(rows)\n",
    "final_df.to_csv(\"Thesis_Topic_Document_Summary_FULL2.csv\", index=False)\n",
    "print(\"âœ… Exported: Thesis_Topic_Document_Summary_FULL.csv (grouped by topic)\")\n",
    "\n",
    "# Print sample of topic labels\n",
    "print(\"\\nðŸ“‹ Sample topic labels:\")\n",
    "print(topic_info[[\"Topic\", \"Name\"]].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800568a-7d30-4123-8027-56da871c428a",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Step 4: Sentiment Analysis and Emotion Recognition\n",
    "## Using twitter-roberta-base-sentiment and twitter-roberta-base-emotion-multilabel-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57763407-316c-46da-961a-c04f2e706be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rcParams\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Set font for Results section globally (optional)\n",
    "# Load SF Pro Display font manually (optional)\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_path = '/Users/leonaweise/Desktop/SF-Pro-Display-Regular.otf'  # <-- path to your font file\n",
    "sfpro_font = font_manager.FontProperties(fname=font_path)\n",
    "\n",
    "# Timestamp for saving output\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Load your reduced BERTopic results file\n",
    "results_df = pd.read_csv(\"bertopic_results_for_sentiment.csv\")\n",
    "\n",
    "\n",
    "# Load sentiment analysis model\n",
    "print(\"Loading sentiment analysis model...\")\n",
    "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "\n",
    "# Load emotion analysis model\n",
    "print(\"Loading emotion analysis model...\")\n",
    "emotion_model_name = \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\"\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)\n",
    "\n",
    "# Sentiment labels\n",
    "sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "\n",
    "def analyze_sentiment(texts, max_length=512):\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Analyzing sentiment\"):\n",
    "        # Convert to string and handle NaN values\n",
    "        if isinstance(text, float) and np.isnan(text):\n",
    "            text = \"\"  # Replace NaN with empty string\n",
    "        else:\n",
    "            text = str(text)  # Convert all types to string\n",
    "\n",
    "        text = text[:10000]  # Now safe to slice\n",
    "        inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = sentiment_model(**inputs)\n",
    "        scores = torch.nn.functional.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
    "        label_id = np.argmax(scores)\n",
    "        results.append({\n",
    "            \"sentiment\": sentiment_labels[label_id],\n",
    "            \"score\": float(scores[label_id]),\n",
    "            \"scores\": {label: float(score) for label, score in zip(sentiment_labels, scores)}\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_emotions(texts, threshold=0.5, max_length=512):\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Analyzing emotions\"):\n",
    "        # Convert to string and handle NaN values\n",
    "        if isinstance(text, float) and np.isnan(text):\n",
    "            text = \"\"  # Replace NaN with empty string\n",
    "        else:\n",
    "            text = str(text)  # Convert all types to string\n",
    "\n",
    "        text = text[:10000]  # Now safe to slice\n",
    "        inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = emotion_model(**inputs)\n",
    "        scores = torch.sigmoid(outputs.logits).detach().numpy()[0]\n",
    "        emotions = [emotion_labels[i] for i, score in enumerate(scores) if score > threshold]\n",
    "        emotion_scores = {emotion_labels[i]: float(score) for i, score in enumerate(scores)}\n",
    "        results.append({\n",
    "            \"emotions\": emotions,\n",
    "            \"scores\": emotion_scores\n",
    "        })\n",
    "    return results\n",
    "def analyze_topic_sentiments_emotions(df, docs_per_topic=None):\n",
    "    if docs_per_topic is None:\n",
    "        docs_per_topic = 50\n",
    "    topic_ids = sorted(df[\"topic\"].unique())\n",
    "    topic_results = {}\n",
    "    for topic_id in topic_ids:\n",
    "        print(f\"\\nAnalyzing Topic {topic_id}\")\n",
    "        topic_docs = df[df[\"topic\"] == topic_id]\n",
    "        if len(topic_docs) > docs_per_topic:\n",
    "            sampled_docs = topic_docs.head(docs_per_topic)  # Take first 50, since documents are sorted by probability, thus the first 50 are the most representative\n",
    "        else:\n",
    "            sampled_docs = topic_docs\n",
    "        texts = sampled_docs[\"document\"].tolist()\n",
    "        sentiment_results = analyze_sentiment(texts)\n",
    "        emotion_results = analyze_emotions(texts)\n",
    "        sentiment_counts = Counter([r[\"sentiment\"] for r in sentiment_results])\n",
    "        sentiment_dist = {s: count/len(sentiment_results) for s, count in sentiment_counts.items()}\n",
    "        all_emotions = [emo for r in emotion_results for emo in r[\"emotions\"]]\n",
    "        emotion_counts = Counter(all_emotions)\n",
    "        emotion_scores = {}\n",
    "        for emotion in emotion_labels:\n",
    "            # Get average score for this emotion across all documents\n",
    "            avg_score = sum(r[\"scores\"].get(emotion, 0) for r in emotion_results) / len(emotion_results)\n",
    "            emotion_scores[emotion] = avg_score\n",
    "\n",
    "        topic_results[topic_id] = {\n",
    "            \"sentiment_distribution\": sentiment_dist,\n",
    "            \"emotion_distribution\": emotion_scores,\n",
    "            \"sample_size\": len(sampled_docs)\n",
    "        }\n",
    "    return topic_results\n",
    "\n",
    "topic_sentiment_emotion_results = analyze_topic_sentiments_emotions(results_df)\n",
    "\n",
    "sentiment_emotion_df = pd.DataFrame()\n",
    "for topic_id, data in topic_sentiment_emotion_results.items():\n",
    "    row = {\"Topic\": topic_id, \"Sample_Size\": data[\"sample_size\"]}\n",
    "    for sentiment, value in data[\"sentiment_distribution\"].items():\n",
    "        row[f\"Sentiment_{sentiment}\"] = value\n",
    "    for emo, value in data[\"emotion_distribution\"].items():\n",
    "        row[f\"Emotion_{emo}\"] = value\n",
    "    sentiment_emotion_df = pd.concat([sentiment_emotion_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Ensure all emotions columns exist\n",
    "for emotion in emotion_labels:\n",
    "    col_name = f\"Emotion_{emotion}\"\n",
    "    if col_name not in sentiment_emotion_df.columns:\n",
    "        sentiment_emotion_df[col_name] = 0.0\n",
    "\n",
    "# Save CSV\n",
    "sentiment_emotion_df.to_csv(f\"topic_sentiment_emotion_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Create a clean summary table of sentiment distributions by topic\n",
    "sentiment_summary = sentiment_emotion_df[['Topic', 'Sample_Size', 'Sentiment_Negative', 'Sentiment_Neutral', 'Sentiment_Positive']].copy()\n",
    "\n",
    "# Sort by topic ID\n",
    "sentiment_summary = sentiment_summary.sort_values('Topic')\n",
    "\n",
    "# Save this simplified table\n",
    "sentiment_summary.to_csv(f\"topic_sentiment_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Create a cleaner visualization - horizontal bar chart of sentiments by topic\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Remove borders (APA 7 style)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Create bar chart for sentiment distribution across all topics\n",
    "topics = sentiment_summary['Topic'].tolist()\n",
    "pos = sentiment_summary['Sentiment_Positive'].tolist()\n",
    "neu = sentiment_summary['Sentiment_Neutral'].tolist()\n",
    "neg = sentiment_summary['Sentiment_Negative'].tolist()\n",
    "\n",
    "indices = list(range(len(topics)))\n",
    "width = 0.8\n",
    "\n",
    "ax.barh(indices, neg, width, label='Negative', color='#d92f0d')\n",
    "ax.barh(indices, neu, width, left=neg, label='Neutral', color='#aaaaaa')\n",
    "ax.barh(indices, pos, width, left=[n+ne for n, ne in zip(neg, neu)], label='Positive', color='#2aad32')\n",
    "\n",
    "ax.set_yticks(indices)\n",
    "ax.set_yticklabels([f\"Topic {t}\" for t in topics], fontproperties=sfpro_font)\n",
    "ax.set_xlabel('Proportion', fontproperties=sfpro_font)\n",
    "\n",
    "# Corrected: save the legend as 'leg'\n",
    "leg = ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, prop=sfpro_font)\n",
    "\n",
    "# Now modify the legend box\n",
    "frame = leg.get_frame()\n",
    "frame.set_edgecolor('black')  # Set legend box border color to black\n",
    "frame.set_linewidth(1.0)      # Optional: make the border slightly visible and clean\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"sentiment_distribution_by_topic_{timestamp}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "def plot_topic_sentiment_emotions(topic_results, top_n_topics=10):\n",
    "    plot_topic_ids = [t for t in topic_results.keys() if t != -1][:top_n_topics]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x_vals = list(range(len(sentiment_labels)))\n",
    "    for topic_id in plot_topic_ids:\n",
    "        sentiment_dist = topic_results[topic_id][\"sentiment_distribution\"]\n",
    "        y = [sentiment_dist.get(label, 0) for label in sentiment_labels]\n",
    "        plt.bar([xi + 0.1*topic_id for xi in x_vals], y, width=0.1, label=f\"Topic {topic_id}\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.title(\"Sentiment Distribution Across Topics\")\n",
    "    plt.xticks(x_vals, sentiment_labels)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"topic_sentiment_distribution_{timestamp}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    x_vals = list(range(len(emotion_labels)))\n",
    "    for topic_id in plot_topic_ids:\n",
    "        emotion_dist = topic_results[topic_id][\"emotion_distribution\"]\n",
    "        y = [emotion_dist.get(label, 0) for label in emotion_labels]\n",
    "        plt.bar([xi + 0.1*topic_id for xi in x_vals], y, width=0.1, label=f\"Topic {topic_id}\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.title(\"Emotion Distribution Across Topics\")\n",
    "    plt.xticks(x_vals, emotion_labels, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"topic_emotion_distribution_{timestamp}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plot_topic_sentiment_emotions(topic_sentiment_emotion_results, top_n_topics=100)\n",
    "print(\"Sentiment and emotion analysis complete. Results saved to CSV and visualizations created.\")\n",
    "\n",
    "# Calculate average sentiment for each topic\n",
    "topic_sentiment_summary = sentiment_emotion_df.groupby('Topic')[['Sentiment_Positive', 'Sentiment_Neutral', 'Sentiment_Negative']].mean().reset_index()\n",
    "\n",
    "# Find the dominant emotions for each topic\n",
    "emotion_cols = [col for col in sentiment_emotion_df.columns if col.startswith('Emotion_')]\n",
    "topic_emotion_summary = sentiment_emotion_df.groupby('Topic')[emotion_cols].mean().reset_index()\n",
    "\n",
    "# For each topic, identify the top 2 emotions\n",
    "top_emotions_by_topic = []\n",
    "for _, row in topic_emotion_summary.iterrows():\n",
    "    topic = row['Topic']\n",
    "    # Get emotion scores for this topic\n",
    "    emotion_scores = {col.replace('Emotion_', ''): row[col] for col in emotion_cols}\n",
    "    # Sort by score and get top 2\n",
    "    top_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "    top_emotions_by_topic.append({\n",
    "        'Topic': topic,\n",
    "        'Primary_Emotion': top_emotions[0][0],\n",
    "        'Primary_Score': top_emotions[0][1],\n",
    "        'Secondary_Emotion': top_emotions[1][0] if len(top_emotions) > 1 else None,\n",
    "        'Secondary_Score': top_emotions[1][1] if len(top_emotions) > 1 else None\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "top_emotions_df = pd.DataFrame(top_emotions_by_topic)\n",
    "\n",
    "# Save the topic sentiment summary\n",
    "topic_sentiment_summary.to_csv(f\"topic_sentiment_averages_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save the top emotions for each topic\n",
    "top_emotions_df.to_csv(f\"topic_dominant_emotions_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Create visualizations for all 34 topics across 11 emotions using three line charts\n",
    "\n",
    "# Define 13 distinct colors\n",
    "primary_colors = [\n",
    "    \"#3498db\",  # Bright Blue\n",
    "    \"#2ecc71\",  # Bright Green\n",
    "    \"#f39c12\",  # Orange-Yellow\n",
    "    \"#f1c40f\",  # Bright Yellow\n",
    "    \"#d92f0d\",  # Strong Red\n",
    "    \"#2c1b96\",  # Deep Blue-Purple\n",
    "    \"#045c2d\",  # Deep Forest Green\n",
    "    \"#d90d5f\",  # Strong Pink\n",
    "    \"#d94e0d\",  # Bright Orange-Red\n",
    "    \"#650dd9\",  # Deep Violet\n",
    "    \"#7289a1\",  # Steel Blue\n",
    "    \"#a6842e\",  # Bronze Yellow\n",
    "    \"#de9a97\",  # Soft Coral Pink\n",
    "]\n",
    "\n",
    "# Prepare emotion columns and labels\n",
    "emotion_cols = [col for col in sentiment_emotion_df.columns if col.startswith('Emotion_')]\n",
    "emotion_labels = [col.replace('Emotion_', '') for col in emotion_cols]\n",
    "\n",
    "# Topic groups: (start_topic, end_topic)\n",
    "topic_groups = [\n",
    "    (0, 11),\n",
    "    (12, 24),\n",
    "    (25, 34)\n",
    "]\n",
    "\n",
    "# Set figure size to A4 landscape\n",
    "figsize = (11.7, 8.3)\n",
    "\n",
    "# Timestamp for saving files\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "for idx, (start_topic, end_topic) in enumerate(topic_groups, 1):\n",
    "    # Filter topics\n",
    "    group_df = sentiment_emotion_df[\n",
    "        (sentiment_emotion_df['Topic'] >= start_topic) & (sentiment_emotion_df['Topic'] <= end_topic)\n",
    "        ]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # Remove the top and right borders for APA\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Plot each topic\n",
    "    for i, (_, row) in enumerate(group_df.iterrows()):\n",
    "        y = [row[f'Emotion_{emo}'] for emo in emotion_labels]\n",
    "        color = primary_colors[i % len(primary_colors)]  # Loop colors safely\n",
    "        ax.plot(emotion_labels, y, label=f'Topic {int(row[\"Topic\"])}',\n",
    "                color=color, linewidth=3, alpha=0.9)\n",
    "\n",
    "    # Customize axes and legend\n",
    "    ax.set_xlabel('Emotion', fontsize=14, fontproperties=sfpro_font)\n",
    "    ax.set_ylabel('Proportion', fontsize=14, fontproperties=sfpro_font)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    leg = ax.legend(loc='upper right', prop=sfpro_font)\n",
    "    leg = ax.legend(loc='upper right', prop=sfpro_font)\n",
    "    frame = leg.get_frame()\n",
    "    frame.set_edgecolor('black')  # Set legend box border color to black\n",
    "    frame.set_linewidth(1.0)  # Optional: set border thickness (1.0 is clean and APA)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    plt.savefig(f\"emotion_distribution_topics_{start_topic}-{end_topic}_sfpro_{timestamp}.png\",\n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Emotion distribution line charts generated and saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
